# Pipeline de DonnÃ©es en Temps RÃ©el pour l'Analyse des Tendances sur Twitter ğŸ“Šâš™ï¸ğŸ“¡

![](https://github.com/akshitvjain/realtime-twitter-trends-analytics/blob/master/images/realtime-twitter-dashboard.gif)

## Motivation ğŸ’¼â±ï¸ğŸ“ˆ

De nos jours, l'analyse des donnÃ©es en temps rÃ©el est essentielle pour les PME comme pour les grandes entreprises, dans des secteurs tels que les services financiers, juridiques, la gestion des opÃ©rations informatiques, le marketing et la publicitÃ©. Cela implique l'analyse de grandes quantitÃ©s de donnÃ©es, en temps rÃ©el comme historiques, afin de prendre des dÃ©cisions Ã©clairÃ©es. ğŸ“ŠğŸ“¡ğŸŒ

Le Big Data se distingue des donnÃ©es classiques par sa vitesse, son volume et sa variÃ©tÃ©. Le dÃ©veloppement d'un pipeline de donnÃ©es distribuÃ© devient donc crucial pour le traitement, le stockage et l'analyse des donnÃ©es en temps rÃ©el, diffÃ©rant ainsi des applications classiques du Big Data. ğŸ”„ğŸ“¦ğŸ“¶

Ce projet personnel vise Ã  appliquer les principes de traitement parallÃ¨le Ã  grande Ã©chelle (cours CS 6240 - NEU) pour crÃ©er un pipeline de traitement en temps rÃ©el en utilisant des outils open source. L'objectif est de capturer, traiter, stocker et analyser efficacement un volume important de donnÃ©es provenant de sources variÃ©es, tout en garantissant la scalabilitÃ© et l'efficacitÃ©. ğŸ› ï¸ğŸ“¥ğŸ“Š

## Description du Projet ğŸŒğŸ¦ğŸ§ 

L'exploitation des tendances Twitter et de l'analyse des sentiments est un excellent cas d'utilisation pour construire un pipeline de donnÃ©es distribuÃ©. Environ 500 millions de tweets sont publiÃ©s chaque jour dans le monde (octobre 2019). Parmi eux, environ 1 %, soit 5 millions de tweets, sont accessibles publiquement. ğŸŒğŸ“‰ğŸ’¬

Ce pipeline de donnÃ©es repose sur les composants suivants : **Apache Kafka** pour l'ingestion des donnÃ©es, **Apache Spark** pour le traitement en temps rÃ©el, **MongoDB** pour le stockage distribuÃ©, et **Apache Drill** pour connecter MongoDB Ã  **Tableau** pour les analyses en temps rÃ©el. ğŸ› ï¸ğŸ’¡ğŸ“¡

Les donnÃ©es Twitter sont rÃ©cupÃ©rÃ©es via l'API de streaming Twitter, envoyÃ©es Ã  Apache Kafka, traitÃ©es par Apache Spark (y compris la classification des sentiments), puis stockÃ©es dans MongoDB. L'analyse de la popularitÃ© et des sentiments des tendances est visualisÃ©e via un tableau de bord crÃ©Ã© avec Tableau. ğŸ“ŠğŸ“²ğŸ§ 

**Remarque :** Apache Drill joue un rÃ´le clÃ© en connectant MongoDB Ã  Tableau. Des dÃ©tails seront donnÃ©s plus loin. ğŸ”ŒğŸ“˜ğŸ“

## Architecture des DonnÃ©es ğŸ§±ğŸ“¶ğŸ“¡

Dans cette architecture, le producteur de tweets en streaming utilise Kafka pour publier les tweets en temps rÃ©el sur le topic "tweets-1". Ensuite, Apache Spark Streaming s'abonne Ã  ce topic pour traiter les tweets. ğŸ”ğŸ—ƒï¸ğŸ’¡

Le moteur Spark utilise Spark Streaming pour effectuer le traitement par lots des tweets entrants. Avant de les stocker, Spark procÃ¨de Ã  une classification des sentiments. Les rÃ©sultats sont ensuite stockÃ©s dans MongoDB. ğŸ’­ğŸ“¤ğŸ—„ï¸

Pour relier MongoDB Ã  Tableau, Apache Drill sert de connecteur, permettant ainsi la crÃ©ation d'un tableau de bord en direct dans Tableau qui fournit des analyses en temps rÃ©el sur la popularitÃ© et le sentiment des tendances Twitter. ğŸ“‰ğŸ“ŠğŸ”—

## Conception du SystÃ¨me ğŸ§°ğŸ§ ğŸ–¥ï¸

### Producteur Kafka de Tweets ğŸŒğŸ’¬ğŸ”„

Ce producteur est responsable de publier les tweets en temps rÃ©el sur le topic "tweets-1" dans le broker Kafka. Il utilise la bibliothÃ¨que **twitter4j** pour se connecter Ã  l'API de Twitter et capturer des tweets en anglais, provenant du monde entier. ğŸŒğŸ“¥ğŸ’»

### Apache Kafka ğŸ’¾ğŸ’¬ğŸ“¨

Kafka est un systÃ¨me de messagerie distribuÃ© publish-subscribe et une file d'attente robuste, conÃ§u pour gÃ©rer de gros volumes de donnÃ©es. Kafka permet la consommation de messages en ligne et hors ligne, garantit la persistance des donnÃ©es sur disque, et rÃ©plique les messages pour assurer leur intÃ©gritÃ©. Il s'intÃ¨gre parfaitement avec Spark pour l'analyse des donnÃ©es en streaming. âš™ï¸ğŸ§µğŸ”

#### DÃ©pendance de Kafka : Apache Zookeeper ğŸ¦“ğŸ”ğŸ”

Zookeeper est un service de configuration et de synchronisation distribuÃ©. Il agit comme interface de coordination entre les brokers Kafka et les consommateurs. Il stocke des mÃ©tadonnÃ©es telles que les topics, brokers, offsets, etc., ce qui permet Ã  Kafka de rester rÃ©silient en cas de panne. ğŸ’½ğŸ§­ğŸ”’

### Apache Spark ğŸ”¥ğŸ§ âš™ï¸

Apache Spark est un framework de calcul distribuÃ© trÃ¨s rapide et flexible. Il propose divers outils : Spark SQL, MLlib (apprentissage automatique), GraphX (traitement de graphes) et Spark Streaming pour l'analyse de donnÃ©es en temps rÃ©el. ğŸ“šğŸ’¡ğŸ“Š

#### Spark Core ğŸ§±ğŸ”ğŸ§®

Le cÅ“ur de Spark repose sur les **RDDs** (Resilient Distributed Datasets), qui sont des collections immuables, partitionnÃ©es et tolÃ©rantes aux pannes. Le graphe de lignÃ©e des RDDs permet de reconstituer les partitions manquantes en cas de dÃ©faillance. ğŸ”„ğŸ§©ğŸ—‚ï¸

#### Spark Streaming ğŸŒŠğŸ§ ğŸ§°

BasÃ© sur Spark Core, Spark Streaming permet l'analyse des donnÃ©es en continu. Il repose sur l'abstraction **DStream** (flux discrÃ©tisÃ©), composÃ© d'une sÃ©rie continue de RDDs. Les transformations peuvent Ãªtre stateless ou stateful, prÃ©parant les tweets bruts Ã  la classification des sentiments. ğŸ”ğŸ“ğŸ“Š

### MongoDB ğŸ—„ï¸ğŸ“¥ğŸ“¡

MongoDB est utilisÃ© comme systÃ¨me de stockage distribuÃ© pour les donnÃ©es traitÃ©es. Les rÃ©sultats de la classification sont stockÃ©s dans MongoDB, assurant une gestion efficace des donnÃ©es. ğŸ”ğŸ“‚ğŸ“ˆ

### Apache Drill ğŸ”ğŸ§ ğŸ”—

Apache Drill est un moteur SQL open source qui permet d'exÃ©cuter des requÃªtes SQL sur des bases de donnÃ©es non relationnelles (comme MongoDB). Il joue un rÃ´le clÃ© pour connecter MongoDB Ã  Tableau. ğŸ› ï¸ğŸ“¡ğŸ’»

### Tableau ğŸ“ˆğŸ¨ğŸ§©

Tableau est un outil de visualisation de donnÃ©es qui utilise les donnÃ©es en temps rÃ©el stockÃ©es dans MongoDB pour crÃ©er un tableau de bord interactif. Ce tableau de bord permet d'analyser les tendances sur Twitter, avec un suivi de leur popularitÃ© et des sentiments en temps rÃ©el. ğŸ§ ğŸ“ŠğŸ“Œ

## Instructions pour Configurer le Pipeline et le Tableau de Bord ğŸ§­ğŸ› ï¸ğŸ”Œ

1. **TÃ©lÃ©charger les composants nÃ©cessaires :**
   - [Zookeeper](https://www.apache.org/dyn/closer.lua/zookeeper/zookeeper-3.5.7/apache-zookeeper-3.5.7-bin.tar.gz)
   - [MongoDB](https://docs.mongodb.com/guides/server/install/)
   - [Apache Kafka](https://archive.apache.org/dist/kafka/2.4.0/kafka_2.12-2.4.0.tgz)
   - [Apache Spark](https://spark.apache.org/downloads.html)
   - [Apache Drill](https://drill.apache.org/docs/installing-drill-on-linux-and-mac-os-x/)

2. **(Optionnel) Installer un environnement de dÃ©veloppement Spark :**
   - [Guide recommandÃ©](https://kaizen.itversity.com/setup-development-environment-intellij-and-scala-big-data-hadoop-and-spark/)

3. **Cloner le dÃ©pÃ´t du projet :**
   - Clonez le dÃ©pÃ´t sur votre machine locale.

4. **CrÃ©er un compte dÃ©veloppeur Twitter :**
   - [S'inscrire ici](https://developer.twitter.com/en/apply-for-access)

5. **Mettre Ã  jour les jetons de l'API Twitter :**
   - Modifier le fichier `oAuth-tokens.txt` dans le rÃ©pertoire `input/` du projet.

6. **DÃ©marrer le serveur Zookeeper :**
   ```bash
   /usr/local/zookeeper/bin/zkServer.sh start
   ```

7. **DÃ©marrer le serveur Kafka :**
   ```bash
   /usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties
   ```

8. **CrÃ©er un topic Kafka :**
   ```bash
   /usr/local/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic tweets-1
   ```

9. **VÃ©rifier la crÃ©ation du topic :**
   ```bash
   /usr/local/kafka/bin/kafka-topics.sh --list --zookeeper localhost:2181
   ```

10. **DÃ©marrer le serveur MongoDB :**

11. **DÃ©marrer Apache Drill en mode distribuÃ© :**
   - Suivre [ce guide](https://drill.apache.org/docs/starting-drill-in-distributed-mode/)

12. **Configurer MongoDB comme plugin de stockage dans Apache Drill :**
   - Suivre [ce guide](https://drill.apache.org/docs/mongodb-storage-plugin/)

13. **ExÃ©cuter KafkaTwitterProducer.java avec les bons arguments.**

14. **ExÃ©cuter KafkaSparkProcessor.scala avec les bons arguments.**

15. **Configurer Tableau pour se connecter Ã  MongoDB via Apache Drill :**
   - Suivre [ce guide](https://help.tableau.com/current/pro/desktop/en-us/examples_apachedrill.htm)

## Outils + IDE ğŸ§°ğŸ§ªğŸ§ 

- [Apache Kafka 2.4.0](https://kafka.apache.org/)
- [Apache Spark 2.4.1](https://spark.apache.org/)
- [Apache Drill 1.17.0](https://drill.apache.org/)
- [MongoDB](https://www.mongodb.com/)
- [Tableau Desktop](https://www.tableau.com/)
- [IntelliJ IDEA](https://www.jetbrains.com/idea/)
- [Java 8](https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html)
- [Scala 2.11.12](https://www.scala-lang.org/download/2.11.12.html)

